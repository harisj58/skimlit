{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27869331",
   "metadata": {},
   "source": [
    "## ðŸ›  Exercises\n",
    "\n",
    "1. Train `model_5` on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:\n",
    "  * [`tf.keras.callbacks.ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) to save the model's best weights only.\n",
    "  * [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) to stop the model from training once the validation loss has stopped improving for ~3 epochs.\n",
    "2. Checkout the [Keras guide on using pretrained GloVe embeddings](https://keras.io/examples/nlp/pretrained_word_embeddings/). Can you get this working with one of our models?\n",
    "  * Hint: You'll want to incorporate it with a custom token [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
    "  * It's up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.\n",
    "3. Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained  embedding for the [TensorFlow Hub BERT PubMed expert](https://tfhub.dev/google/experts/bert/pubmed/2) (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?\n",
    "  * Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the [TensorFlow Hub guide](https://tfhub.dev/google/experts/bert/pubmed/2)).\n",
    "  * Does the BERT model beat the results mentioned in this paper? https://arxiv.org/pdf/1710.06071.pdf \n",
    "4. What happens if you were to merge our `line_number` and `total_lines` features for each sequence? For example, created a `X_of_Y` feature instead? Does this effect model performance?\n",
    "  * Another example: `line_number=1` and `total_lines=11` turns into `line_of_X=1_of_11`.\n",
    "5. Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * `PREDICTED_LABEL`: `SEQUENCE`\n",
    "  * ...\n",
    "    * You can find your own unstrcutured RCT abstract from PubMed or try this one from: [*Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection*](https://pubmed.ncbi.nlm.nih.gov/22244707/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e0b138d",
   "metadata": {},
   "source": [
    "We begin with some standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "076e6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from helper_functions import calculate_results\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c302232a",
   "metadata": {},
   "source": [
    "We require the PubMed dataset so we import the corresponding GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678733cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "!cd pubmed-rct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0facc552",
   "metadata": {},
   "source": [
    "The following cell will provide us all the prerequisites we need to perform our excercises without being interrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f26321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning Projects\\skimlit\\env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=((TensorSpec(shape=(None, 15), dtype=tf.float32, name=None), TensorSpec(shape=(None, 20), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "\n",
    "def get_lines(filename):\n",
    "  \"\"\"\n",
    "  Reads filename (a text filename) and returns the lines of text as a list.\n",
    "\n",
    "  Args:\n",
    "    filename: a string containing the target filepath.\n",
    "\n",
    "  Returns:\n",
    "    A list of strings with one string per line from the target filename.\n",
    "  \"\"\"\n",
    "  with open(filename, 'r') as file:\n",
    "    return file.readlines()\n",
    "\n",
    "def preprocess_text_with_line_numbers(filename):\n",
    "  \"\"\"\n",
    "  Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "  Takes in filename, reads its contents and sorts through each line,\n",
    "  extracting things like the target label, the text of the sentence,\n",
    "  how many sentences are in the current abstract and what sentence,\n",
    "  number the target line is.\n",
    "  \"\"\"\n",
    "  input_lines = get_lines(filename) # get all lines from filename\n",
    "  abstract_lines = \"\" # create an empty abstract\n",
    "  abstract_samples = [] # create an empty list of abstracts\n",
    "\n",
    "  # Loop through each line in the target line\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"):  # check to see if line is an ID line\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\" # reset abstract string\n",
    "    elif line.isspace():  # check to see if line is a new line\n",
    "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "      # Iterate through each line in a single abstract and count them at the same time\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {}  # create empty dict to store data from line\n",
    "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "        line_data['target'] = target_text_split[0]  # get target label\n",
    "        line_data['text'] = target_text_split[1].lower()  # get target text and lower it\n",
    "        line_data['line_number'] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "        line_data['total_lines'] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
    "        abstract_samples.append(line_data) # how many total lines are in the abstract? (start from 0)\n",
    "\n",
    "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "      abstract_lines += line\n",
    "\n",
    "  return abstract_samples\n",
    "\n",
    "train_df = pd.DataFrame(preprocess_text_with_line_numbers(data_dir+'train.txt'))\n",
    "val_df = pd.DataFrame(preprocess_text_with_line_numbers(data_dir+'dev.txt'))\n",
    "test_df = pd.DataFrame(preprocess_text_with_line_numbers(data_dir+'test.txt'))\n",
    "\n",
    "train_sentences = train_df['text'].tolist()\n",
    "val_sentences = val_df['text'].tolist()\n",
    "test_sentences = test_df['text'].tolist()\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False) # we want non-sparse matrix\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df.target.to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df.target.to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df.target.to_numpy().reshape(-1, 1))\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels_encoded = le.fit_transform(train_df.target.to_numpy())\n",
    "val_labels_encoded = le.transform(val_df.target.to_numpy())\n",
    "test_labels_encoded = le.transform(test_df.target.to_numpy())\n",
    "\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name='universal_sentence_encoder')\n",
    "\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))\n",
    "\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "char_length = [len(sentence) for sentence in train_sentences]\n",
    "\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "output_seq_char_len = int(np.percentile(char_length, 95))\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
    "char_vec = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                             output_sequence_length=output_seq_char_len,\n",
    "                             name='character_vectorizer')\n",
    "char_vec.adapt(train_chars)\n",
    "\n",
    "char_vocab = char_vec.get_vocabulary()\n",
    "char_embed = layers.Embedding(input_dim=len(char_vocab),  # number of different characters\n",
    "                              output_dim=25,  # this is the size of the char embedding in the paper: https://arxiv.org/pdf/1612.05251.pdf (Figure 1)\n",
    "                              mask_zero=True,\n",
    "                              name='char_embed')\n",
    "\n",
    "train_line_numbers_one_hot = tf.one_hot(train_df.line_number.to_numpy(), depth=15)\n",
    "val_line_numbers_one_hot = tf.one_hot(val_df.line_number.to_numpy(), depth=15)\n",
    "test_line_numbers_one_hot = tf.one_hot(test_df.line_number.to_numpy(), depth=15)\n",
    "\n",
    "train_lines_total_one_hot = tf.one_hot(train_df.total_lines.to_numpy(), depth=20)\n",
    "val_lines_total_one_hot = tf.one_hot(val_df.total_lines.to_numpy(), depth=20)\n",
    "test_lines_total_one_hot = tf.one_hot(test_df.total_lines.to_numpy(), depth=20)\n",
    "\n",
    "train_tribrid_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
    "                                                         train_lines_total_one_hot,\n",
    "                                                         train_sentences,\n",
    "                                                         train_chars))\n",
    "train_tribrid_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
    "train_tribrid_dataset = tf.data.Dataset.zip((train_tribrid_data, train_tribrid_labels))\n",
    "train_tribrid_dataset = train_tribrid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_tribrid_data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
    "                                                       val_lines_total_one_hot,\n",
    "                                                       val_sentences, \n",
    "                                                       val_chars))\n",
    "val_tribrid_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_tribrid_dataset = tf.data.Dataset.zip((val_tribrid_data, val_tribrid_labels))\n",
    "val_tribrid_dataset = val_tribrid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_tribrid_data = tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,\n",
    "                                                        test_lines_total_one_hot,\n",
    "                                                        test_sentences, \n",
    "                                                        test_chars))\n",
    "test_tribrid_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
    "test_tribrid_dataset = tf.data.Dataset.zip((test_tribrid_data, test_tribrid_labels))\n",
    "test_tribrid_dataset = test_tribrid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train_tribrid_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63c4f814",
   "metadata": {},
   "source": [
    "### Excercise 1: Training model until it stops improving\n",
    "\n",
    "With the use of `tensorflow.keras.callbacks` we can stop training our model once our certain specified criteria has been met. In this case, it will be the stagnation of `val_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608e7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Token inputs\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_outputs = layers.Dense(128, activation='relu')(token_embeddings)\n",
    "token_model = tf.keras.Model(token_inputs,\n",
    "                             token_outputs)\n",
    "\n",
    "# 2. Char inputs\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "char_vectors = char_vec(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
    "char_model = tf.keras.Model(char_inputs,\n",
    "                            char_bi_lstm)\n",
    "\n",
    "# 3. Line number model\n",
    "line_number_inputs = layers.Input(shape=(15, ), dtype=tf.float32)\n",
    "line_number_outputs = layers.Dense(32, activation='relu')(line_number_inputs)\n",
    "line_number_model = tf.keras.Model(line_number_inputs,\n",
    "                                   line_number_outputs)\n",
    "\n",
    "# 4. Total lines model\n",
    "total_lines_inputs = layers.Input(shape=(20, ), dtype=tf.float32)\n",
    "total_lines_outputs = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
    "total_lines_model = tf.keras.Model(total_lines_inputs,\n",
    "                                   total_lines_outputs)\n",
    "\n",
    "# 5. Combine models 1 and 2\n",
    "combined_embeddings = layers.Concatenate(name='char_token_hybrid_embedding')([token_model.output, char_model.output])\n",
    "z = layers.Dense(256, activation='relu')(combined_embeddings)\n",
    "z = layers.Dropout(0.5)(z)\n",
    "\n",
    "# 6. Combine positional embedding with combined token and char embeddings\n",
    "tribrid_embeddings = layers.Concatenate(name='char_token_positional_embeddings')([line_number_model.output,\n",
    "                                                                                 total_lines_model.output,\n",
    "                                                                                 z])\n",
    "\n",
    "# 7. Create output layer\n",
    "output_layer = layers.Dense(5, activation='softmax', name='output_layer')(tribrid_embeddings)\n",
    "\n",
    "# 8. Get it all together\n",
    "model_6 = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                                 total_lines_model.input,\n",
    "                                 token_model.input,\n",
    "                                 char_model.inputs],\n",
    "                         outputs=output_layer,\n",
    "                         name='model_6_tribrid_enhanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "401504e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3679b19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6_tribrid_enhanced\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " character_vectorizer (TextVect  (None, 290)         0           ['input_2[0][0]']                \n",
      " orization)                                                                                       \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['input_1[0][0]']                \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 290, 25)      700         ['character_vectorizer[0][0]']   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          65664       ['universal_sentence_encoder[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 48)           9600        ['char_embed[0][0]']             \n",
      "                                                                                                  \n",
      " char_token_hybrid_embedding (C  (None, 176)         0           ['dense[0][0]',                  \n",
      " oncatenate)                                                      'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 15)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 256)          45312       ['char_token_hybrid_embedding[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           512         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           672         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " char_token_positional_embeddin  (None, 320)         0           ['dense_1[0][0]',                \n",
      " gs (Concatenate)                                                 'dense_2[0][0]',                \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " output_layer (Dense)           (None, 5)            1605        ['char_token_positional_embedding\n",
      "                                                                 s[0][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,921,889\n",
      "Trainable params: 124,065\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the summary\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2c6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '.\\\\tmp\\\\checkpoint'\n",
    "mc = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                     save_weights_only=True,\n",
    "                     monitor='accuracy',\n",
    "                     mode='max',\n",
    "                     save_best_only=True)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0.01,\n",
    "                   patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49832c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_6.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91213db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "562/562 [==============================] - 34s 60ms/step - loss: 0.9545 - accuracy: 0.8221 - val_loss: 0.9448 - val_accuracy: 0.8248\n",
      "Epoch 2/20\n",
      "562/562 [==============================] - 41s 73ms/step - loss: 0.9336 - accuracy: 0.8387 - val_loss: 0.9366 - val_accuracy: 0.8388\n",
      "Epoch 3/20\n",
      "562/562 [==============================] - 32s 58ms/step - loss: 0.9358 - accuracy: 0.8334 - val_loss: 0.9291 - val_accuracy: 0.8371\n",
      "Epoch 4/20\n",
      "562/562 [==============================] - 38s 68ms/step - loss: 0.9305 - accuracy: 0.8396 - val_loss: 0.9219 - val_accuracy: 0.8428\n",
      "Epoch 5/20\n",
      "562/562 [==============================] - 22s 40ms/step - loss: 0.9301 - accuracy: 0.8408 - val_loss: 0.9164 - val_accuracy: 0.8434\n",
      "Epoch 6/20\n",
      "562/562 [==============================] - 19s 34ms/step - loss: 0.9339 - accuracy: 0.8347 - val_loss: 0.9156 - val_accuracy: 0.8474\n",
      "Epoch 7/20\n",
      "562/562 [==============================] - 22s 40ms/step - loss: 0.9242 - accuracy: 0.8441 - val_loss: 0.9144 - val_accuracy: 0.8408\n",
      "Epoch 8/20\n",
      "562/562 [==============================] - 23s 41ms/step - loss: 0.9197 - accuracy: 0.8464 - val_loss: 0.9034 - val_accuracy: 0.8574\n",
      "Epoch 9/20\n",
      "562/562 [==============================] - 38s 67ms/step - loss: 0.9228 - accuracy: 0.8447 - val_loss: 0.9159 - val_accuracy: 0.8484\n",
      "Epoch 10/20\n",
      "562/562 [==============================] - 28s 51ms/step - loss: 0.9186 - accuracy: 0.8477 - val_loss: 0.9044 - val_accuracy: 0.8514\n",
      "Epoch 11/20\n",
      "  7/562 [..............................] - ETA: 16s - loss: 0.9150 - accuracy: 0.8600WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 11240 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 11240 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - 4s 7ms/step - loss: 0.9150 - accuracy: 0.8600 - val_loss: 0.9101 - val_accuracy: 0.8428\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history_6 = model_6.fit(train_tribrid_dataset,\n",
    "                        steps_per_epoch=int(0.1*len(train_tribrid_dataset)),\n",
    "                        epochs=20,\n",
    "                        validation_data=val_tribrid_dataset,\n",
    "                        validation_steps=int(0.1*len(val_tribrid_dataset)),\n",
    "                        callbacks=[mc, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb13d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 15s 16ms/step - loss: 0.9068 - accuracy: 0.8493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9067788124084473, 0.8492652177810669]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset\n",
    "model_6.evaluate(val_tribrid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a494781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58933675, 0.10183831, 0.01632273, 0.26716176, 0.02534041],\n",
       "       [0.57850176, 0.10878776, 0.07371843, 0.21760412, 0.02138789],\n",
       "       [0.38493866, 0.09261087, 0.08450082, 0.38327268, 0.054677  ],\n",
       "       ...,\n",
       "       [0.02926422, 0.07242577, 0.02314727, 0.03077097, 0.84439176],\n",
       "       [0.01994521, 0.3650633 , 0.05783086, 0.02192453, 0.53523606],\n",
       "       [0.08063398, 0.82472867, 0.0431503 , 0.02469845, 0.02678857]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_6_pred_probs = model_6.predict(val_tribrid_dataset)\n",
    "model_6_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f06c423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 0, ..., 4, 4, 1], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn predictions into labels\n",
    "model_6_preds = tf.argmax(model_6_pred_probs, axis=1)\n",
    "model_6_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d4fae06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 84.92651926386866,\n",
       " 'precision': 0.8518644675250031,\n",
       " 'recall': 0.8492651926386866,\n",
       " 'f1': 0.8456816522143424}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all metrics\n",
    "model_6_results = calculate_results(val_labels_encoded, model_6_preds)\n",
    "model_6_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
